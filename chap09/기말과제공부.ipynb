{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FastFeatureDetector 특징 검출 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(kp)= 98\n",
      "len(kp2)= 867\n"
     ]
    }
   ],
   "source": [
    "# 0901.py\n",
    "import cv2\n",
    "import numpy as np\n",
    " \n",
    "src = cv2.imread('c:/data/chessBoard.jpg')\n",
    "gray= cv2.cvtColor(src,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#1 꼭짓점 검출\n",
    "##fastF = cv2.FastFeatureDetector_create()\n",
    "# fastF =cv2.FastFeatureDetector.create()\n",
    "fastF =cv2.FastFeatureDetector.create(threshold=30) # 100\n",
    "kp = fastF.detect(gray) \n",
    "dst = cv2.drawKeypoints(gray, kp, None, color=(0,0,255))\n",
    "print('len(kp)=', len(kp))\n",
    "cv2.imshow('dst',  dst)\n",
    "\n",
    "#2 주변 값을 억제하지 않은 경우\n",
    "fastF.setNonmaxSuppression(False)\n",
    "kp2 = fastF.detect(gray)\n",
    "dst2 = cv2.drawKeypoints(src, kp2, None, color=(0,0,255))\n",
    "print('len(kp2)=', len(kp2))\n",
    "cv2.imshow('dst2',  dst2)\n",
    "\n",
    "#3\n",
    "dst3 = src.copy()\n",
    "points = cv2.KeyPoint_convert(kp)\n",
    "points = np.int32(points)\n",
    "\n",
    "for cx, cy in points:\n",
    "    cv2.circle(dst3, (cx, cy), 3, color=(255, 0, 0), thickness=1)\n",
    "cv2.imshow('dst3',  dst3)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3 MSER GRAY or Color 에서 주변보다 더 밝거나 더 어두운 여역으로 임계값의 범위에서 안정적인 영역을 특징으로 검출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(kp)= 202\n"
     ]
    }
   ],
   "source": [
    "# 0903.py\n",
    "import cv2\n",
    "import numpy as np\n",
    " \n",
    "src = cv2.imread('c:/data/chessBoard.jpg')\n",
    "gray= cv2.cvtColor(src,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#1 중심점 검출\n",
    "mserF = cv2.MSER_create(10)  # cv2.MSER.create(10)\n",
    "kp= mserF.detect(gray)\n",
    "print('len(kp)=', len(kp))\n",
    "dst = cv2.drawKeypoints(gray, kp, None, color=(0,0,255))\n",
    "cv2.imshow('dst',  dst)\n",
    "\n",
    "#2 특징 영역의 좌표 계산 후 출력\n",
    "dst2 = dst.copy()\n",
    "regions, bboxes = mserF.detectRegions(gray)\n",
    "hulls = [cv2.convexHull(p.reshape(-1, 1, 2)) for p in regions]\n",
    "cv2.polylines(dst2, hulls, True, (0, 255, 0))\n",
    "cv2.imshow('dst2',  dst2)\n",
    "\n",
    "#3 각 영역의 좌표를 통해 파란색 타원과 초록색 사각형 그리기\n",
    "dst3 = dst.copy()\n",
    "for i, pts in enumerate(regions):\n",
    "    box = cv2.fitEllipse(pts)\n",
    "    cv2.ellipse(dst3, box,  (255,0,0),1) # 타원\n",
    "    x, y, w, h = bboxes[i]\n",
    "    cv2.rectangle(dst3, (x, y), (x+w, y+h), (0,255,0)) # 사각형\n",
    "cv2.imshow('dst3',  dst3)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.6 ORB 특징 검출 및 디스크립터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(kp)= 97\n",
      "len(filtered_kp)= 27\n",
      "des.shape= (27, 32)\n",
      "des= [[ 14  92 102 187  55 243 188 230  29  79  84 183 108  76  54 201 255  51\n",
      "  151 131 166 180  57 255 103  95 218 131 151  93 189 215]\n",
      " [110  18 235 236 164 114  94  82   5 142 223  34  73 189 179 104 217 182\n",
      "  151 161 140 163  32 136 211 255 238 137 150 250 116  67]\n",
      " [149  89 233 199 172  86 239 216 196 192 214  66 113 252 245  98 120 100\n",
      "   62 251 201 165 127 170 123  40 175  16  53 187 112 100]\n",
      " [154  95  46 188   6 132 191 118 132 142 214 167 109 239  51  67 110 241\n",
      "  159 227 232 167  34 238 243  67 238 129  54 253  53 123]\n",
      " [ 34 216 104 136 111 209 154 200 143  69 156 163  44 222 102 196 206  57\n",
      "  147 193   6   3  42 241  18  84 138 251  23  14  61 223]\n",
      " [ 63  11  98 206  93 151 238 112 197  76 221   6 105 200  55 224 120 168\n",
      "  151 235 204 163  20 220 115 109 162 137  55 190 120 198]\n",
      " [ 66 218  56 186  47 193 152 236 143  29 148 163 172 206  98  64 207  19\n",
      "  191 129 198  11  74 241 208  81 187 251  20  12  61  91]\n",
      " [ 28  13  98 169  22 195 235 240  20  68  94 167  57  77 178  64 122 168\n",
      "  151 235 136 184   5 190 242  71 170 129  63 143  60 208]\n",
      " [146 130  37 187  44 193 152 111  30  31 145  51  44  62  56 193 205 147\n",
      "  191 129  58  79  75 213 153  83 218 211  20  12 180  75]\n",
      " [108  32 156 250 141 108  81 136  34 183 173  24 214 115  75  26 145  86\n",
      "  121  26  26  70 219  32 207 247  85  63 128  83  86  34]\n",
      " [170 215 120 222 244  20 191 254  23  63 156 246  24 205 254  67 255 218\n",
      "  159 226 242 111  56 247 127 235 191 230  23 217 252  51]\n",
      " [  2 139  26 171 150 128 186 231  14   1  90 163 172 170  52  64 238  59\n",
      "  147 209  22 136   2 245 108  37 138 195 149  10 185 139]\n",
      " [ 35 222  64 136 118 157 250 222  31  77 124 199  41  12 190  64 255 139\n",
      "  151 212 135  56   4 255  62  92 178 226  61 141  88 221]\n",
      " [178 101 118 239 239  99 255  79 175  38 255 155 141 143  52  74 238 223\n",
      "  255 195 120 255 235 243 247 248 154 179  55 223 229 235]\n",
      " [ 70 155 241 233 174  64 179 140 143 189  16 163 204 253 108  98 203  95\n",
      "  191  81  18 104 235 241  77  80 239 239 150  59 181 243]\n",
      " [ 40  44  56 101  65 111 113  55 105  46 110  24 179  49   0  16 128  30\n",
      "   64   8  96 184 113  10  71 249  16  16  74 200  66 170]\n",
      " [110  16 130 228  77  20 112  80 131 170 157  15  12 157  98 206 236 141\n",
      "  179 161   4  55 144 192  87 209 195  47 135  47 124 211]\n",
      " [154 213  55 242 135 196 180 177 140  90  19 115 252  95  40 227 230 115\n",
      "  159  67  99 193  26 229 225 194 206 130  30  12 181 123]\n",
      " [180 116 189 255  12   4 221 216 146 150 245  26 183 223 105  43 205  89\n",
      "   19  98  42 102 250 170 193 219 223  60  17 121   6 170]\n",
      " [ 72 185 155 229  48 204 113  54 174 201  74  64 182 151   0 112 128  52\n",
      "  124  72  81 137 177  12 237 245  36   0   0   2   5  59]\n",
      " [130  20  30 252 150  38  41  55 148  19 150  18 156  55  48 105 111  55\n",
      "   59 169  49 197  49 206  65 171 251 132 180 104 161  96]\n",
      " [143 149 112 203 191 213 187 184 223  73  29  50 220 220 252 228 247  27\n",
      "  191 121 179  45  62 247  47  93  15 206 151  15 249  53]\n",
      " [226 151 132 190 172  33  21 143 158  31 144  55 228 227  42  75 197  82\n",
      "   47   1 122  71  75 179 236 178 254 191 144  88 148  11]\n",
      " [  2  13  90 254  86 230 251  98  23   4 126   2 105 247  32  64 126 251\n",
      "  131 192   2 154   2 255 119 239 138 139  53 139  52 170]\n",
      " [ 55  45  98 158  85 243 186 104  95  64 249   3 108 232  54 224 248 171\n",
      "  147 197  22 187  30 255  49 118 139 107  85  15 121 198]\n",
      " [ 45 123  41  13 120  95  19  50  65 236 244 237 154  20 135 180 204 116\n",
      "  212 102 205 162 113  28 103 121  33 190  74 189  86 117]\n",
      " [  3  72  10 154  87 147 170  72  22   0  81  34 104 202  51 192 248  40\n",
      "  130 195  38 179  10 255  52  38 139 137  93  43  57 134]]\n"
     ]
    }
   ],
   "source": [
    "# 0906.py\n",
    "import cv2\n",
    "import numpy as np\n",
    "#1\n",
    "def distance(f1, f2):    \n",
    "    x1, y1 = f1.pt\n",
    "    x2, y2 = f2.pt\n",
    "    return np.sqrt((x2 - x1)**2+ (y2 - y1)**2)\n",
    "\n",
    "def filteringByDistance(kp, distE=0.5):\n",
    "    size = len(kp)\n",
    "    mask = np.arange(1,size+1).astype(np.bool8) # all True   \n",
    "    for i, f1 in enumerate(kp):\n",
    "        if not mask[i]:\n",
    "            continue\n",
    "        else: # True\n",
    "            for j, f2 in enumerate(kp):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                if distance(f1, f2)<distE:\n",
    "                    mask[j] = False\n",
    "    np_kp = np.array(kp)\n",
    "    return list(np_kp[mask])\n",
    "\n",
    "#2\n",
    "src = cv2.imread('c:/data/temp/pad.jpg')\n",
    "src = cv2.resize(src, (512, 512))\n",
    "# src = cv2.imread('c:/data/cornerTest.jpg')\n",
    "##src = cv2.imread('./data/chessBoard.jpg')\n",
    "gray= cv2.cvtColor(src,cv2.COLOR_BGR2GRAY)\n",
    "gray = cv2.GaussianBlur(gray, (5, 5), 0.0)\n",
    "\n",
    "##orbF = cv2.ORB_create()          \n",
    "orbF = cv2.ORB_create(scoreType=1)\n",
    "kp= orbF.detect(gray)\n",
    "print('len(kp)=', len(kp))\n",
    "dst = cv2.drawKeypoints(gray, kp, None, color=(0,0,255))   \n",
    "cv2.imshow('dst',  dst)\n",
    "\n",
    "#3\n",
    "kp = sorted(kp, key=lambda f: f.response, reverse=True)\n",
    "filtered_kp = list(filter(lambda f: f.response>50, kp))\n",
    "filtered_kp = filteringByDistance(kp, 10)\n",
    "print('len(filtered_kp)=', len(filtered_kp))\n",
    "\n",
    "kp, des = orbF.compute(gray, filtered_kp)\n",
    "print('des.shape=', des.shape)\n",
    "print('des=', des)\n",
    "\n",
    "#4\n",
    "dst2 = cv2.drawKeypoints(gray, filtered_kp, None, color=(0,0,255))  \n",
    "for f in filtered_kp:\n",
    "    x, y = f.pt\n",
    "    size = f.size\n",
    "    rect = ((x, y), (size, size), f.angle)\n",
    "    box = cv2.boxPoints(rect).astype(np.int32)\n",
    "    cv2.polylines(dst2, [box], True, (0,255,0), 2)\n",
    "    cv2.circle(dst2, (round(x), round(y)), round(f.size/2), (255,0,0), 2)\n",
    "cv2.imshow('dst2',  dst2)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.8 BRISK 특징 검출 및 디스크립터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(kp)= 35\n",
      "len(filtered_kp)= 16\n",
      "des.shape= (16, 64)\n",
      "des= [[252 255 239 ... 141 128 137]\n",
      " [  0   0   0 ... 199  78 205]\n",
      " [240 255 239 ... 141   8 185]\n",
      " ...\n",
      " [255 157 243 ... 199 207 223]\n",
      " [255 255 255 ... 205 156 255]\n",
      " [255 191 227 ... 152 153  11]]\n"
     ]
    }
   ],
   "source": [
    "# 0908.py\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "#1\n",
    "def distance(f1, f2):    \n",
    "    x1, y1 = f1.pt\n",
    "    x2, y2 = f2.pt\n",
    "    return np.sqrt((x2 - x1)**2+ (y2 - y1)**2)\n",
    "\n",
    "def filteringByDistance(kp, distE=0.5):\n",
    "    size = len(kp)\n",
    "    mask = np.arange(1,size+1).astype(np.bool8) # all True   \n",
    "    for i, f1 in enumerate(kp):\n",
    "        if not mask[i]:\n",
    "            continue\n",
    "        else: # True\n",
    "            for j, f2 in enumerate(kp):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                if distance(f1, f2)<distE:\n",
    "                    mask[j] = False\n",
    "    np_kp = np.array(kp)\n",
    "    return list(np_kp[mask])\n",
    "    \n",
    "#2\n",
    "src = cv2.imread('c:/data/temp/pad.jpg')\n",
    "src = cv2.resize(src, (512, 512))\n",
    "gray= cv2.cvtColor(src,cv2.COLOR_BGR2GRAY)\n",
    "gray = cv2.GaussianBlur(gray, (5, 5), 0.0)\n",
    "\n",
    "briskF = cv2.BRISK_create()\n",
    "kp= briskF.detect(gray)\n",
    "print('len(kp)=', len(kp))\n",
    "dst = cv2.drawKeypoints(gray, kp, None, color=(0,0,255))   \n",
    "cv2.imshow('dst',  dst)\n",
    "\n",
    "#3\n",
    "kp = sorted(kp, key=lambda f: f.response, reverse=True)\n",
    "filtered_kp = list(filter(lambda f: f.response>50, kp))\n",
    "filtered_kp = filteringByDistance(kp, 10)\n",
    "print('len(filtered_kp)=', len(filtered_kp))\n",
    "\n",
    "kp, des = briskF.compute(gray, filtered_kp)\n",
    "print('des.shape=', des.shape)\n",
    "print('des=', des)\n",
    "\n",
    "#4\n",
    "dst2 = cv2.drawKeypoints(gray, filtered_kp, None, color=(0,0,255))  \n",
    "for f in filtered_kp:\n",
    "    x, y = f.pt\n",
    "    size = f.size\n",
    "    rect = ((x, y), (size, size), f.angle)\n",
    "    box = cv2.boxPoints(rect).astype(np.int32)\n",
    "    cv2.polylines(dst2, [box], True, (0,255,0), 2)\n",
    "    cv2.circle(dst2, (round(x), round(y)), round(f.size/2), (255,0,0), 2)\n",
    "cv2.imshow('dst2',  dst2)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ORB, BRISK 매칭"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(matches)= 224\n",
      "matches[0]=(queryIdx:0, trainIdx:7, distance:288.5220947265625)\n",
      "matches[1]=(queryIdx:167, trainIdx:7, distance:356.2218322753906)\n",
      "matches[2]=(queryIdx:56, trainIdx:119, distance:372.7036437988281)\n",
      "len(good_matches)= 224\n"
     ]
    }
   ],
   "source": [
    "# 0912.py\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "#1    \n",
    "src1 = cv2.imread('c:/data/temp/mouse1.jpg') # 'cup1.jpg'\n",
    "src2 = cv2.imread('c:/data/temp/mouse3.jpg') # 'cup2.jpg'\n",
    "src1 = cv2.resize(src1, (300, 300))\n",
    "src2 = cv2.resize(src2, (300, 300))\n",
    "img1= cv2.cvtColor(src1,cv2.COLOR_BGR2GRAY)\n",
    "img2= cv2.cvtColor(src2,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#2-1 ORB의 특징점(kp) 검출, 디스크립터(des) 계산\n",
    "# orbF   = cv2.ORB_create(nfeatures=100)\n",
    "# kp1, des1 = orbF.detectAndCompute(img1, None)\n",
    "# kp2, des2 = orbF.detectAndCompute(img2, None)\n",
    "\n",
    "#2-2 BRICKF의 특징점(kp) 검출, 디스크립터(des) 계산\n",
    "briskF = cv2.BRISK_create()\n",
    "kp1, des1 = briskF.detectAndCompute(img1, None)\n",
    "kp2, des2 = briskF.detectAndCompute(img2, None)\n",
    "\n",
    "#3-1 BF 매칭으로 확인\n",
    "# bf = cv2.BFMatcher_create(cv2.NORM_HAMMING, crossCheck=True)\n",
    "# matches = bf.match(des1,des2)\n",
    "\t\n",
    "#3-2 FlannBase 매칭으로 확인\n",
    "flan = cv2.FlannBasedMatcher_create() \n",
    "matches = flan.match(np.float32(des1),np.float32(des2))\n",
    "\t\n",
    "#4 투영 계산\n",
    "matches = sorted(matches, key = lambda m: m.distance)\n",
    "print('len(matches)=', len(matches))\n",
    "for i, m in enumerate(matches[:3]):\n",
    "        print('matches[{}]=(queryIdx:{}, trainIdx:{}, distance:{})'.format(\n",
    "            i, m. queryIdx, m.trainIdx, m.distance))\n",
    "\t\n",
    "minDist = matches[0].distance\n",
    "good_matches = list(filter(lambda m: m.distance<5*minDist, matches))\n",
    "print('len(good_matches)=', len(good_matches))\n",
    "if len(good_matches) < 5:\n",
    "    print('sorry, too small good matches')\n",
    "    exit()\n",
    "    \n",
    "dst = cv2.drawMatches(img1,kp1,img2,kp2,good_matches, None, flags=2)\n",
    "cv2.imshow('dst',  dst)\n",
    "\n",
    "#5 특징점을 리스트에 저장 후 mask 변환\n",
    "src1_pts = np.float32([ kp1[m.queryIdx].pt for m in good_matches])\n",
    "src2_pts = np.float32([ kp2[m.trainIdx].pt for m in good_matches])\n",
    "\n",
    "H, mask = cv2.findHomography(src1_pts, src2_pts, cv2.RANSAC, 3.0)#cv2.LMEDS\n",
    "mask_matches = mask.ravel().tolist() # list(mask.flatten())\n",
    "\n",
    "#6 영상의 모서리 좌표 저장\n",
    "h,w = img1.shape\n",
    "pts = np.float32([ [0,0],[0,h-1],[w-1,h-1],[w-1,0] ]).reshape(-1,1,2)\n",
    "pts2 = cv2.perspectiveTransform(pts, H)\n",
    "src2 = cv2.polylines(src2,[np.int32(pts2)],True,(255,0, 0),2)\n",
    "        \n",
    "draw_params=dict(matchColor = (0,255,0), singlePointColor = None,\n",
    "                 matchesMask = mask_matches,  flags = 2)                 \n",
    "dst2 = cv2.drawMatches(src1,kp1,src2,kp2, good_matches, None,**draw_params)  \n",
    "cv2.imshow('dst2',  dst2)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VLAD 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def extract_local_descriptors(image):\n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Create a SIFT object\n",
    "    sift = cv2.xfeatures2d.SIFT_create()\n",
    "\n",
    "    # Detect and compute local descriptors\n",
    "    keypoints, descriptors = sift.detectAndCompute(gray, None)\n",
    "\n",
    "    return descriptors\n",
    "# List of image filenames\n",
    "path = 'c:/data/temp/'\n",
    "image_filenames = [ path + 'bottle1.jpg', path + 'bottle2.jpg', path + 'bottle3.jpg', path + 'bottle4.jpg']\n",
    "\n",
    "# Extract local descriptors from the images\n",
    "descriptors_list = []\n",
    "for image_filename in image_filenames:\n",
    "    image = cv2.imread(image_filename)\n",
    "    image = cv2.resize(image, (32, 32))\n",
    "    descriptors = extract_local_descriptors(image)\n",
    "    descriptors_list.append(descriptors)\n",
    "\n",
    "# Concatenate the descriptors into a single numpy array\n",
    "descriptors_concatenated = np.concatenate(descriptors_list)\n",
    "\n",
    "# Perform k-means clustering to obtain visual words\n",
    "K = 2  # Number of visual words\n",
    "term_crit = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)\n",
    "flags = cv2.KMEANS_RANDOM_CENTERS\n",
    "_, labels, visual_words = cv2.kmeans(descriptors_concatenated, K, None, term_crit, 10, flags)\n",
    "\n",
    "# Initialize the VLAD vector\n",
    "vlad_vector = np.zeros(K * descriptors_concatenated.shape[1])\n",
    "\n",
    "# Compute the residuals and accumulate them to the VLAD vector\n",
    "for descriptors in descriptors_list:\n",
    "    _, labels, _ = cv2.kmeans(descriptors, K, None, term_crit, 10, flags)\n",
    "    for i in range(len(labels)):\n",
    "        vlad_vector[int(labels[i]) * descriptors_concatenated.shape[1]:(int(labels[i]) + 1) * descriptors_concatenated.shape[1]] += descriptors[i] - visual_words[int(labels[i])]\n",
    "\n",
    "# L2 normalize the VLAD vector\n",
    "vlad_vector /= np.linalg.norm(vlad_vector)\n",
    "\n",
    "print(len(vlad_vector))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vlad 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def extract_local_descriptors(image):\n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Create a SIFT object\n",
    "    sift = cv2.xfeatures2d.SIFT_create()\n",
    "\n",
    "    # Detect and compute local descriptors\n",
    "    keypoints, descriptors = sift.detectAndCompute(gray, None)\n",
    "\n",
    "    return descriptors\n",
    "\n",
    "# List of image filenames\n",
    "path = 'c:/data/temp/'\n",
    "image_filenames = [ path + 'mouse1.jpg', path + 'mouse2.jpg', path + 'mouse3.jpg', path + 'mouse4.jpg', path + 'mouse5.jpg']\n",
    "\n",
    "\n",
    "# Extract local descriptors from the images\n",
    "descriptors_list = []\n",
    "for image_filename in image_filenames:\n",
    "    image = cv2.imread(image_filename)\n",
    "    image = cv2.resize(image, (32, 32))\n",
    "    descriptors = extract_local_descriptors(image)\n",
    "    descriptors_list.append(descriptors)\n",
    "\n",
    "# Concatenate the descriptors into a single numpy array\n",
    "descriptors_concatenated = np.concatenate(descriptors_list)\n",
    "\n",
    "# Perform k-means clustering to obtain visual words\n",
    "num_clusters = 2  # Number of visual words\n",
    "kmeans = KMeans(n_clusters=num_clusters)\n",
    "kmeans.fit(descriptors_concatenated)\n",
    "visual_words = kmeans.cluster_centers_\n",
    "\n",
    "# Initialize the VLAD vector\n",
    "vlad_vector = np.zeros(num_clusters * descriptors_concatenated.shape[1])\n",
    "\n",
    "# Compute the residuals and accumulate them to the VLAD vector\n",
    "for descriptors in descriptors_list:\n",
    "    labels = kmeans.predict(descriptors)\n",
    "    for i in range(len(labels)):\n",
    "        vlad_vector[labels[i]*descriptors_concatenated.shape[1]:(labels[i]+1)*descriptors_concatenated.shape[1]] += descriptors[i] - visual_words[labels[i]]\n",
    "\n",
    "# L2 normalize the VLAD vector\n",
    "vlad_vector /= np.linalg.norm(vlad_vector)\n",
    "\n",
    "print(vlad_vector, len(vlad_vector))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vlad로 디스크립터를 연산한 이미지 검색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "imgSize = (300, 300)\n",
    "\n",
    "def compute_vlad(descriptors, visual_words):\n",
    "    k = visual_words.shape[0]  # Number of visual words\n",
    "    d = descriptors.shape[1]   # Dimension of descriptors\n",
    "    n = descriptors.shape[0]   # Number of descriptors\n",
    "\n",
    "    # Assign each descriptor to the nearest visual word\n",
    "    _, label, _ = cv2.kmeans(descriptors, k, None, criteria=(cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 0.01), attempts=3, flags=cv2.KMEANS_RANDOM_CENTERS)\n",
    "\n",
    "    # Compute residuals and accumulate them for each visual word\n",
    "    vlad_vector = np.zeros((k, d), dtype=np.float32)\n",
    "    for i in range(k):\n",
    "        # Find descriptors assigned to the current visual word\n",
    "        assigned_descriptors = descriptors[label.ravel() == i]\n",
    "\n",
    "        # Subtract the visual word from the assigned descriptors to compute residuals\n",
    "        residuals = assigned_descriptors - visual_words[i]\n",
    "\n",
    "        # Accumulate the residuals\n",
    "        vlad_vector[i] = np.sum(residuals, axis=0)\n",
    "\n",
    "    # L2 normalize the VLAD vector\n",
    "    vlad_vector = cv2.normalize(vlad_vector, None, alpha=1, norm_type=cv2.NORM_L2)\n",
    "\n",
    "    # Flatten the VLAD vector\n",
    "    vlad_vector = vlad_vector.flatten()\n",
    "\n",
    "    return vlad_vector\n",
    "\n",
    "def compute_vlad_descriptor(image_path, sift, visual_words):\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    image = cv2.resize(image, imgSize)\n",
    "    # Detect keypoints and compute descriptors using SIFT\n",
    "    keypoints, descriptors = sift.detectAndCompute(image, None)\n",
    "\n",
    "    # Compute the VLAD descriptor\n",
    "    if descriptors is not None:\n",
    "        vlad_descriptor = compute_vlad(descriptors, visual_words)\n",
    "    else:\n",
    "        vlad_descriptor = None\n",
    "\n",
    "    return vlad_descriptor\n",
    "\n",
    "def search_similar_images(query_image_path, database_image_paths, num_results):\n",
    "    sift = cv2.xfeatures2d.SIFT_create()\n",
    "\n",
    "    # Compute VLAD descriptors for database images\n",
    "    database_descriptors = []\n",
    "    for image_path in database_image_paths:\n",
    "        vlad_descriptor = compute_vlad_descriptor(image_path, sift, visual_words)\n",
    "        if vlad_descriptor is not None:\n",
    "            database_descriptors.append(vlad_descriptor)\n",
    "\n",
    "    query_image = cv2.imread(query_image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    query_image = cv2.resize(query_image, (300, 300))  # Resize query image to 300x300\n",
    "    _, query_descriptors = sift.detectAndCompute(query_image, None)\n",
    "\n",
    "    # Create FLANN-based matcher\n",
    "    flann = cv2.FlannBasedMatcher()\n",
    "\n",
    "    # Iterate over the database images\n",
    "    similar_images = []\n",
    "    for image_path in database_image_paths:\n",
    "        # Compute descriptors for the database image\n",
    "        database_image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "        database_image = cv2.resize(database_image, (300, 300))  # Resize database image to 300x300\n",
    "        _, database_descriptors = sift.detectAndCompute(database_image, None)\n",
    "\n",
    "        # Match descriptors using FLANN-based matcher\n",
    "        matches = flann.knnMatch(query_descriptors, database_descriptors, k=2)\n",
    "\n",
    "        # Apply ratio test to filter good matches\n",
    "        good_matches = []\n",
    "        for m, n in matches:\n",
    "            if m.distance < 0.7 * n.distance:\n",
    "                good_matches.append(m)\n",
    "\n",
    "        # Store the number of good matches for the image\n",
    "        num_good_matches = len(good_matches)\n",
    "        similar_images.append((image_path, num_good_matches))\n",
    "\n",
    "    # Sort the images based on the number of good matches\n",
    "    similar_images.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Retrieve the top similar images\n",
    "    similar_images = [image_path for image_path, _ in similar_images[:num_results]]\n",
    "\n",
    "    return similar_images\n",
    "\n",
    "# Set the path to the query image\n",
    "path = 'c:/data/temp/'\n",
    "query_image_path = path + 'mouse5.jpg' # 검색 이미지\n",
    "\n",
    "# 데이터 이미지\n",
    "database_image_paths = [path + 'mouse1.jpg', path + 'mouse2.jpg', path + 'mouse3.jpg', path + 'mouse4.jpg', path + 'mouse7.jpg', \n",
    "                        path + 'bottle1.jpg', path + 'bottle2.jpg', path + 'bottle3.jpg', path + 'bottle4.jpg', path + 'bottle5.jpg',\n",
    "                        path + 'flower1.jpg', path + 'flower2.jpg', path + 'flower3.jpg', path + 'flower4.jpg', path + 'flower5.jpg']\n",
    "\n",
    "# Set the number of similar images to retrieve\n",
    "num_results = 7\n",
    "\n",
    "# Perform the image search\n",
    "similar_images = search_similar_images(query_image_path, database_image_paths, num_results)\n",
    "\n",
    "# Display the similar images\n",
    "for image_path in similar_images:\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.resize(image, imgSize)\n",
    "    cv2.imshow('Similar Image', image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 89\u001b[0m\n\u001b[0;32m     86\u001b[0m num_results \u001b[39m=\u001b[39m \u001b[39m7\u001b[39m\n\u001b[0;32m     88\u001b[0m \u001b[39m# Perform the image search\u001b[39;00m\n\u001b[1;32m---> 89\u001b[0m similar_images \u001b[39m=\u001b[39m search_similar_images(query_image_path, database_image_paths, num_results)\n\u001b[0;32m     91\u001b[0m \u001b[39m# Display the similar images\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[39mfor\u001b[39;00m image_path \u001b[39min\u001b[39;00m similar_images:\n",
      "Cell \u001b[1;32mIn[6], line 67\u001b[0m, in \u001b[0;36msearch_similar_images\u001b[1;34m(query_image_path, database_image_paths, num_results)\u001b[0m\n\u001b[0;32m     64\u001b[0m database_vlad_vectors \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(database_vlad_vectors)\n\u001b[0;32m     66\u001b[0m \u001b[39m# Add database vectors to Faiss index\u001b[39;00m\n\u001b[1;32m---> 67\u001b[0m index\u001b[39m.\u001b[39;49madd(database_vlad_vectors)\n\u001b[0;32m     69\u001b[0m \u001b[39m# Search for similar images\u001b[39;00m\n\u001b[0;32m     70\u001b[0m _, indices \u001b[39m=\u001b[39m index\u001b[39m.\u001b[39msearch(np\u001b[39m.\u001b[39marray([query_vlad_vector]), num_results)\n",
      "File \u001b[1;32mc:\\Python38\\lib\\site-packages\\faiss\\class_wrappers.py:228\u001b[0m, in \u001b[0;36mhandle_Index.<locals>.replacement_add\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Adds vectors to the index.\u001b[39;00m\n\u001b[0;32m    216\u001b[0m \u001b[39mThe index must be trained before vectors can be added to it.\u001b[39;00m\n\u001b[0;32m    217\u001b[0m \u001b[39mThe vectors are implicitly numbered in sequence. When `n` vectors are\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[39m    `dtype` must be float32.\u001b[39;00m\n\u001b[0;32m    225\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    227\u001b[0m n, d \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mshape\n\u001b[1;32m--> 228\u001b[0m \u001b[39massert\u001b[39;00m d \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39md\n\u001b[0;32m    229\u001b[0m x \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mascontiguousarray(x, dtype\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mfloat32\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    230\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_c(n, swig_ptr(x))\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "imgSize = (300, 300)\n",
    "\n",
    "def extract_sift_features(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.resize(image, imgSize)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    sift = cv2.xfeatures2d.SIFT_create()\n",
    "    keypoints, descriptors = sift.detectAndCompute(gray, None)\n",
    "\n",
    "    return descriptors\n",
    "\n",
    "\n",
    "def perform_kmeans(descriptors, num_clusters=16):\n",
    "    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 0.1)\n",
    "    _, labels, cluster_centers = cv2.kmeans(descriptors, num_clusters, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)\n",
    "\n",
    "    return cluster_centers\n",
    "\n",
    "\n",
    "def compute_vlad(descriptors, cluster_centers):\n",
    "    vlad_vector = np.zeros((cluster_centers.shape[0], cluster_centers.shape[1]), dtype=np.float32)\n",
    "    index = faiss.IndexFlatIP(cluster_centers.shape[1])\n",
    "    index.add(cluster_centers)\n",
    "    _, indices = index.search(descriptors.reshape(-1, cluster_centers.shape[1]), 1)\n",
    "    for i, visual_word_index in enumerate(indices):\n",
    "        vlad_vector[visual_word_index] += descriptors[i] - cluster_centers[visual_word_index]\n",
    "\n",
    "    vlad_vector = vlad_vector.flatten()\n",
    "    vlad_vector /= np.linalg.norm(vlad_vector)\n",
    "\n",
    "    return vlad_vector\n",
    "\n",
    "\n",
    "def search_similar_images(query_image_path, database_image_paths, num_results=5):\n",
    "    # Extract SIFT features from the query image\n",
    "    query_descriptors = extract_sift_features(query_image_path)\n",
    "\n",
    "    # Perform K-means clustering on the database descriptors\n",
    "    database_descriptors = []\n",
    "    for image_path in database_image_paths:\n",
    "        descriptors = extract_sift_features(image_path)\n",
    "        database_descriptors.extend(descriptors)\n",
    "\n",
    "    database_descriptors = np.array(database_descriptors)\n",
    "    cluster_centers = perform_kmeans(database_descriptors)\n",
    "\n",
    "    # Compute VLAD vectors for the query descriptors\n",
    "    query_vlad_vector = compute_vlad(query_descriptors, cluster_centers)\n",
    "\n",
    "    # Initialize Faiss index\n",
    "    index = faiss.IndexFlatIP(cluster_centers.shape[1])\n",
    "\n",
    "    # Compute VLAD vectors for the database descriptors\n",
    "    database_vlad_vectors = []\n",
    "    for descriptors in database_descriptors:\n",
    "        vlad_vector = compute_vlad(descriptors, cluster_centers)\n",
    "        database_vlad_vectors.append(vlad_vector)\n",
    "\n",
    "    database_vlad_vectors = np.array(database_vlad_vectors)\n",
    "\n",
    "    # Add database vectors to Faiss index\n",
    "    index.add(database_vlad_vectors)\n",
    "\n",
    "    # Search for similar images\n",
    "    _, indices = index.search(np.array([query_vlad_vector]), num_results)\n",
    "    similar_images = [database_image_paths[i] for i in indices[0]]\n",
    "\n",
    "    return similar_images\n",
    "\n",
    "\n",
    "# Set the path to the query image\n",
    "path = 'c:/data/temp/'\n",
    "query_image_path = path + 'mouse5.jpg' # 검색 이미지\n",
    "\n",
    "# 데이터 이미지\n",
    "database_image_paths = [path + 'mouse1.jpg', path + 'mouse2.jpg', path + 'mouse3.jpg', path + 'mouse4.jpg', path + 'mouse7.jpg', \n",
    "                        path + 'bottle1.jpg', path + 'bottle2.jpg', path + 'bottle3.jpg', path + 'bottle4.jpg', path + 'bottle5.jpg',\n",
    "                        path + 'flower1.jpg', path + 'flower2.jpg', path + 'flower3.jpg', path + 'flower4.jpg', path + 'flower5.jpg']\n",
    "\n",
    "# Set the number of similar images to retrieve\n",
    "num_results = 7\n",
    "\n",
    "# Perform the image search\n",
    "similar_images = search_similar_images(query_image_path, database_image_paths, num_results)\n",
    "\n",
    "# Display the similar images\n",
    "for image_path in similar_images:\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.resize(image, imgSize)\n",
    "    cv2.imshow('Similar Image', image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 이미지 검색, 잘 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "imgSize = (64, 64)\n",
    "\n",
    "# num_results 출력할 비슷한 이미지 개수\n",
    "def detect_similar_images(query_image_path, database_image_paths, num_results=5):\n",
    "    # Load the query image\n",
    "    query_image = cv2.imread(query_image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    query_image = cv2.resize(query_image, imgSize)\n",
    "    \n",
    "    # Create the feature detector (SIFT)\n",
    "    sift = cv2.xfeatures2d.SIFT_create()\n",
    "\n",
    "    # Detect keypoints and compute descriptors for the query image\n",
    "    query_keypoints, query_descriptors = sift.detectAndCompute(query_image, None)\n",
    "    \n",
    "    # Create a brute-force matcher\n",
    "    bf = cv2.BFMatcher()\n",
    "\n",
    "    # List to store the similarity scores and corresponding image paths\n",
    "    similarity_scores = []\n",
    "    similar_image_paths = []\n",
    "\n",
    "    # Iterate over the database images\n",
    "    for image_path in database_image_paths:\n",
    "        # Load the database image\n",
    "        database_image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "        database_image = cv2.resize(database_image, imgSize)\n",
    "        # Detect keypoints and compute descriptors for the database image\n",
    "        database_keypoints, database_descriptors = sift.detectAndCompute(database_image, None)\n",
    "\n",
    "        # Match the descriptors between the query and database images\n",
    "        matches = bf.match(query_descriptors, database_descriptors)\n",
    "\n",
    "        # Calculate the similarity score as the total distance of matched keypoints\n",
    "        similarity_score = sum([match.distance for match in matches])\n",
    "\n",
    "        # Store the similarity score and image path\n",
    "        similarity_scores.append(similarity_score)\n",
    "        similar_image_paths.append(image_path)\n",
    "\n",
    "    # Sort the images based on similarity scores (lower score indicates higher similarity)\n",
    "    sorted_indices = np.argsort(similarity_scores)\n",
    "\n",
    "    # Retrieve the top similar images\n",
    "    similar_images = []\n",
    "    for i in range(num_results):\n",
    "        image_path = similar_image_paths[sorted_indices[i]]\n",
    "        image = cv2.imread(image_path)\n",
    "        similar_images.append(image)\n",
    "\n",
    "    return similar_images\n",
    "\n",
    "# Set the path to the query image\n",
    "path = 'c:/data/temp/'\n",
    "query_image_path = path + 'bottle7.jpg' # 검색 이미지\n",
    "\n",
    "# 데이터 이미지\n",
    "database_image_paths = [path + 'mouse1.jpg', path + 'mouse2.jpg', path + 'mouse3.jpg', path + 'mouse4.jpg', path + 'mouse7.jpg', \n",
    "                        path + 'bottle1.jpg', path + 'bottle2.jpg', path + 'bottle3.jpg', path + 'bottle4.jpg', path + 'bottle5.jpg']\n",
    "\n",
    "# Detect similar images\n",
    "similar_images = detect_similar_images(query_image_path, database_image_paths)\n",
    "\n",
    "# Display the similar images\n",
    "for i in range(len(similar_images)):\n",
    "    image = cv2.resize(similar_images[i], (300, 300))\n",
    "    cv2.imshow('Similar Image'+str(i), image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### faiss 추가 버전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (22,12) into shape (22,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[61], line 79\u001b[0m\n\u001b[0;32m     75\u001b[0m database_image_paths \u001b[39m=\u001b[39m [ path \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmouse1.jpg\u001b[39m\u001b[39m'\u001b[39m, path \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmouse2.jpg\u001b[39m\u001b[39m'\u001b[39m, path \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmouse3.jpg\u001b[39m\u001b[39m'\u001b[39m, path \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmouse4.jpg\u001b[39m\u001b[39m'\u001b[39m, path \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmouse5.jpg\u001b[39m\u001b[39m'\u001b[39m, \n\u001b[0;32m     76\u001b[0m                         path \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39mbottle1.jpg\u001b[39m\u001b[39m'\u001b[39m, path \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39mbottle2.jpg\u001b[39m\u001b[39m'\u001b[39m, path \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39mbottle3.jpg\u001b[39m\u001b[39m'\u001b[39m, path \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39mbottle4.jpg\u001b[39m\u001b[39m'\u001b[39m, path \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39mbottle5.jpg\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     78\u001b[0m \u001b[39m# Detect similar images\u001b[39;00m\n\u001b[1;32m---> 79\u001b[0m similar_images \u001b[39m=\u001b[39m detect_similar_images(query_image_path, database_image_paths, num_results\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m)\n\u001b[0;32m     81\u001b[0m \u001b[39m# Display the similar images\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[39mfor\u001b[39;00m image \u001b[39min\u001b[39;00m similar_images:\n",
      "Cell \u001b[1;32mIn[61], line 57\u001b[0m, in \u001b[0;36mdetect_similar_images\u001b[1;34m(query_image_path, database_image_paths, num_results)\u001b[0m\n\u001b[0;32m     54\u001b[0m     similar_image_paths\u001b[39m.\u001b[39mappend(image_path)\n\u001b[0;32m     56\u001b[0m \u001b[39m# Sort the images based on similarity scores (higher score indicates higher similarity)\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m sorted_indices \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49margsort(similarity_scores)[::\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m     59\u001b[0m \u001b[39m# Retrieve the top similar images\u001b[39;00m\n\u001b[0;32m     60\u001b[0m similar_images \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36margsort\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Python38\\lib\\site-packages\\numpy\\core\\fromnumeric.py:1120\u001b[0m, in \u001b[0;36margsort\u001b[1;34m(a, axis, kind, order)\u001b[0m\n\u001b[0;32m   1012\u001b[0m \u001b[39m@array_function_dispatch\u001b[39m(_argsort_dispatcher)\n\u001b[0;32m   1013\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39margsort\u001b[39m(a, axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, kind\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, order\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m   1014\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1015\u001b[0m \u001b[39m    Returns the indices that would sort an array.\u001b[39;00m\n\u001b[0;32m   1016\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1118\u001b[0m \n\u001b[0;32m   1119\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1120\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapfunc(a, \u001b[39m'\u001b[39;49m\u001b[39margsort\u001b[39;49m\u001b[39m'\u001b[39;49m, axis\u001b[39m=\u001b[39;49maxis, kind\u001b[39m=\u001b[39;49mkind, order\u001b[39m=\u001b[39;49morder)\n",
      "File \u001b[1;32mc:\\Python38\\lib\\site-packages\\numpy\\core\\fromnumeric.py:54\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     52\u001b[0m bound \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(obj, method, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m     53\u001b[0m \u001b[39mif\u001b[39;00m bound \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 54\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapit(obj, method, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m     56\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     57\u001b[0m     \u001b[39mreturn\u001b[39;00m bound(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n",
      "File \u001b[1;32mc:\\Python38\\lib\\site-packages\\numpy\\core\\fromnumeric.py:43\u001b[0m, in \u001b[0;36m_wrapit\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m:\n\u001b[0;32m     42\u001b[0m     wrap \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(asarray(obj), method)(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m     44\u001b[0m \u001b[39mif\u001b[39;00m wrap:\n\u001b[0;32m     45\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(result, mu\u001b[39m.\u001b[39mndarray):\n",
      "\u001b[1;31mValueError\u001b[0m: could not broadcast input array from shape (22,12) into shape (22,)"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "imgSize = (64, 64)\n",
    "\n",
    "def extract_local_descriptors(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    sift = cv2.xfeatures2d.SIFT_create()\n",
    "    keypoints, descriptors = sift.detectAndCompute(gray, None)\n",
    "    return keypoints, descriptors\n",
    "\n",
    "def detect_similar_images(query_image_path, database_image_paths, num_results=5):\n",
    "    # Load the query image\n",
    "    query_image = cv2.imread(query_image_path)\n",
    "    query_image = cv2.resize(query_image, imgSize)\n",
    "    query_keypoints, query_descriptors = extract_local_descriptors(query_image)\n",
    "\n",
    "    # Create an index for Faiss\n",
    "    d = query_descriptors.shape[1]  # Dimension of the descriptors\n",
    "    index = faiss.IndexFlatIP(d)    # Inner product (dot product) distance measure\n",
    "\n",
    "    # Add the query descriptor to the index\n",
    "    index.add(np.float32(query_descriptors))\n",
    "\n",
    "    # List to store the similarity scores and corresponding image paths\n",
    "    similarity_scores = []\n",
    "    similar_image_paths = []\n",
    "\n",
    "    # Iterate over the database images\n",
    "    for image_path in database_image_paths:\n",
    "        # Load the database image\n",
    "        database_image = cv2.imread(image_path)\n",
    "        database_image = cv2.resize(database_image, imgSize)\n",
    "        database_keypoints, database_descriptors = extract_local_descriptors(database_image)\n",
    "\n",
    "        # Perform a k-nearest neighbor search using Faiss\n",
    "        k = 1  # Number of nearest neighbors to retrieve\n",
    "        _, indices = index.search(np.float32(database_descriptors), k)\n",
    "\n",
    "        # Check if indices are within the valid range\n",
    "        valid_indices = indices[indices < len(database_descriptors)]\n",
    "        if len(valid_indices) == 0:\n",
    "            continue\n",
    "\n",
    "        # Reshape the database descriptors to remove the extra dimension\n",
    "        database_descriptors = np.squeeze(database_descriptors[valid_indices])\n",
    "\n",
    "        # Compute the similarity score as the inner product (dot product) between query and database descriptors\n",
    "        similarity_score = np.dot(query_descriptors, database_descriptors.T)\n",
    "\n",
    "        # Store the similarity score and image path\n",
    "        similarity_scores.append(similarity_score)\n",
    "        similar_image_paths.append(image_path)\n",
    "\n",
    "    # Sort the images based on similarity scores (higher score indicates higher similarity)\n",
    "    sorted_indices = np.argsort(similarity_scores)[::-1]\n",
    "\n",
    "    # Retrieve the top similar images\n",
    "    similar_images = []\n",
    "    for i in range(num_results):\n",
    "        if i >= len(sorted_indices):\n",
    "            break\n",
    "        image_path = similar_image_paths[sorted_indices[i]]\n",
    "        image = cv2.imread(image_path)\n",
    "        similar_images.append(image)\n",
    "\n",
    "    return similar_images\n",
    "\n",
    "# Set the path to the query image\n",
    "path = 'c:/data/temp/'\n",
    "query_image_path = path + 'bottle7.jpg' # 검색 이미지\n",
    "\n",
    "# 데이터 이미지\n",
    "database_image_paths = [ path + 'mouse1.jpg', path + 'mouse2.jpg', path + 'mouse3.jpg', path + 'mouse4.jpg', path + 'mouse5.jpg', \n",
    "                        path + 'bottle1.jpg', path + 'bottle2.jpg', path + 'bottle3.jpg', path + 'bottle4.jpg', path + 'bottle5.jpg']\n",
    "\n",
    "# Detect similar images\n",
    "similar_images = detect_similar_images(query_image_path, database_image_paths, num_results=5)\n",
    "\n",
    "# Display the similar images\n",
    "for image in similar_images:\n",
    "    cv2.imshow('Similar Image', image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
